"""update_pipeline.py

Ğ˜Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ°Ğ·Ñƒ CCE.

Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾:
* Ğ¯Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº ÑˆĞ°Ğ³Ğ¾Ğ² ÑĞ¾ ÑÑ‡Ñ‘Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ¼ (Â«Step 1/5 â€¦Â»), Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ±Ñ‹Ğ»Ğ¾ Ğ²Ğ¸Ğ´Ğ½Ğ¾, Ğ³Ğ´Ğµ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ.
* Ğ•ÑĞ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… URL Ğ½ĞµÑ‚ â€” Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ INFOâ€‘ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ **Ğ´Ğ¾** Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ.

Ğ—Ğ°Ğ¿ÑƒÑĞº:
    python update_pipeline.py                 # ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹
    python update_pipeline.py -n links2.txt   # ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ» Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑÑ‹Ğ»Ğ¾Ğº
"""

from __future__ import annotations

import argparse
import datetime as dt
import logging
import shutil
import subprocess
import sys
from pathlib import Path
from typing import List

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_NEW_FILE   = "new_links.txt"
DEFAULT_CLEAN_FILE = "clean_links_unique.txt"
DEFAULT_TMP_FILE   = "new_links_unique.txt"

EXTRACT = "extract.py"
CLEAN   = "clean_text_extractor.py"
CHUNK   = "chunker.py"
DEDUPER = "deduper.py"          # optional
ENRICH  = "enricher.py"

CHUNK_SIZE = 500
OVERLAP    = 30
DEDUP_THRESHOLD = 0.9            # None â†’ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑˆĞ°Ğ³ Ğ´ĞµĞ´ÑƒĞ¿Ğ°

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ’ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run(cmd: List[str | Path]) -> None:
    """Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½ÑÑ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ Ğ¸ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ñ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ² Ğ»Ğ¾Ğ³.
    ĞŸÑ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº str/Path, ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² str Ğ´Ğ»Ñ subprocess.
    """
    cmd_str = [str(c) for c in cmd]
    logging.info("$ %s", " ".join(cmd_str))
    print(f"\nğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ: {' '.join(cmd_str)}")

    proc = subprocess.Popen(
        cmd_str,
        text=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        encoding='utf-8',
        errors='replace'
    )
    assert proc.stdout is not None
    for line in proc.stdout:
        line = line.rstrip()
        logging.debug(line)
        print(f"  {line}")
    proc.wait()
    if proc.returncode != 0:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ (ĞºĞ¾Ğ´ {proc.returncode})")
        raise RuntimeError(f"Command failed: {' '.join(cmd_str)} â†’ {proc.returncode}")
    print("âœ… ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾\n")


def diff_links(new_path: Path, clean_path: Path, tmp_path: Path) -> int:
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ² tmp_path Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğµ URL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½ĞµÑ‚ Ğ² clean_path.
    Ğ•ÑĞ»Ğ¸ clean_path Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ â€“ ÑÑ‡Ğ¸Ñ‚Ğ°ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ±Ğ°Ğ·Ğ° Ğ¿ÑƒÑÑ‚Ğ°.
    """
    if clean_path.exists():
        clean = {u.strip() for u in clean_path.read_text(encoding="utf-8").splitlines() if u.strip()}
    else:
        logging.warning("%s not found; treating as empty master list", clean_path)
        clean = set()
    added = 0
    with new_path.open("r", encoding="utf-8") as src, tmp_path.open("w", encoding="utf-8") as dst:
        for url in src:
            url = url.strip()
            if url and url not in clean:
                dst.write(url + "\n"); added += 1
    return added


def append_file(src: Path, dst: Path) -> None:
    if not src.exists():
        return
    with src.open("rb") as s, dst.open("ab") as d:
        shutil.copyfileobj(s, d)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> None:
    parser = argparse.ArgumentParser(description="Incremental update of CCE pipeline")
    parser.add_argument("-n", "--new",   default=DEFAULT_NEW_FILE,   help="file with raw new links")
    parser.add_argument("-c", "--clean", default=DEFAULT_CLEAN_FILE, help="master unique links list")
    parser.add_argument("-q", "--quiet", action="store_true", help="suppress INFO logs, show only WARNING")
    parser.add_argument("--skip-dedupe",  action="store_true", help="skip semantic dedupe step even if available")
    args = parser.parse_args()

    logging.basicConfig(level=logging.WARNING if args.quiet else logging.INFO,
                        format="%(levelname)s: %(message)s")

    print("\nğŸ“‹ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ñ‹ CCE")
    print("=" * 50)

    root = Path.cwd()
    new_path, clean_path, tmp_path = root / args.new, root / args.clean, root / DEFAULT_TMP_FILE

    if not new_path.exists():
        print(f"âŒ Ğ¤Ğ°Ğ¹Ğ» {new_path} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
        logging.critical("File %s not found", new_path); sys.exit(1)

    print(f"\nğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğµ URL Ğ¸Ğ· {new_path}")
    added = diff_links(new_path, clean_path, tmp_path)
    print(f"ğŸ“Š ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {added} Ğ½Ğ¾Ğ²Ñ‹Ñ… URL")

    # â”€â”€ Early exit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if added == 0:
        print("\nâœ¨ ĞĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… URL - Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµĞ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ")
        logging.info("No new links â†’ pipeline finished BEFORE enrichment âœ‹")
        return

    ts = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    meta        = root / f"meta_{ts}.jsonl"
    meta_clean  = root / f"meta_clean_{ts}.jsonl"
    chunks      = root / f"chunks_{ts}.jsonl"
    enriched    = root / f"enriched_{ts}.jsonl"

    step = 1; total = 5 + (0 if args.skip_dedupe else 1)

    # Step 1: extract ---------------------------------------------------
    print(f"\nğŸ“ Ğ¨Ğ°Ğ³ {step}/{total}: Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    print("-" * 50)
    run([sys.executable, EXTRACT, "--input", tmp_path, "--output", meta])

    # Step 2: clean -----------------------------------------------------
    print(f"\nğŸ§¹ Ğ¨Ğ°Ğ³ {step}/{total}: ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ°")
    print("-" * 50)
    run([sys.executable, CLEAN, "--input", meta, "--output", meta_clean])

    # Step 3: chunk -----------------------------------------------------
    print(f"\nâœ‚ï¸ Ğ¨Ğ°Ğ³ {step}/{total}: Ğ Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸")
    print("-" * 50)
    run([sys.executable, CHUNK,
         "--input", meta_clean,
         "--output", chunks,
         "--chunk_size", str(CHUNK_SIZE),
         "--overlap", str(OVERLAP)])

    # Step 4: dedupe (optional) ----------------------------------------
    chunks_to_enrich = chunks
    if not args.skip_dedupe and Path(DEDUPER).exists() and DEDUP_THRESHOLD:
        print(f"\nğŸ”„ Ğ¨Ğ°Ğ³ {step}/{total}: Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ")
        print("-" * 50)
        dedup_chunks = root / f"chunks_dedup_{ts}.jsonl"
        run([sys.executable, DEDUPER,
             "-i", chunks,
             "-o", dedup_chunks,
             "-t", str(DEDUP_THRESHOLD)])
        chunks_to_enrich = dedup_chunks
    else:
        print("\nâ© ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑˆĞ°Ğ³ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸")

    # Step 5: enrich ----------------------------------------------------
    print(f"\nâœ¨ Ğ¨Ğ°Ğ³ {step}/{total}: ĞĞ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    print("-" * 50)
    run([sys.executable, ENRICH, "--input", chunks_to_enrich, "--output", enriched])

    # Step 6: append to master -----------------------------------------
    print(f"\nğŸ’¾ Ğ¨Ğ°Ğ³ {step}/{total}: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ")
    print("-" * 50)
    append_file(tmp_path,          clean_path)
    append_file(chunks_to_enrich,  root / "chunks.jsonl")
    append_file(enriched,          root / "enriched_chunks.jsonl")

    print("\nğŸ‰ ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾!")
    logging.info("Pipeline finished âœ…. Master files updated.")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.critical("Fatal: %s", e, exc_info=True)
        sys.exit(1)

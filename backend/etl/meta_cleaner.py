import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import asyncio
from datetime import datetime
from openai import AsyncOpenAI
from dotenv import load_dotenv
from sqlalchemy import select, or_

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ‘Ğ” Ğ¸Ğ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹
from models.models import Page, Link, DatasetSettings
from core.db import async_session_maker

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ĞĞ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼ OpenAI
client = AsyncOpenAI(api_key=OPENAI_API_KEY)

async def gpt_clean(text: str, prompt: str, model: str) -> str:
    response = await client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt.format(text=text)}],
        max_tokens=32,
        temperature=0
    )
    return response.choices[0].message.content.strip()

async def process_pages(dataset_id: int):
    async with async_session_maker() as session:
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
        settings_q = await session.execute(
            select(DatasetSettings).where(DatasetSettings.dataset_id == dataset_id)
        )
        settings = settings_q.scalar_one_or_none()
        model = settings.gpt_model if settings else "gpt-3.5-turbo"
        prompt = "ĞŸÑ€Ğ¸Ğ²ĞµĞ´Ğ¸ Ğ² Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¹ Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ²Ğ¸Ğ´: \"{text}\""

        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ Ğ½ĞµĞ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑĞ¼Ğ¸
        result = await session.execute(
            select(Page)
            .join(Link, Link.id == Page.link_id)
            .where(Link.dataset_id == dataset_id)
            .where(
                or_(
                    Page.clean_author.is_(None) & Page.raw_author.isnot(None) & (~Page.author_needs_review),
                    Page.clean_date.is_(None) & Page.raw_date.isnot(None) & (~Page.date_needs_review),
                    Page.clean_category.is_(None) & Page.raw_category.isnot(None) & (~Page.category_needs_review)
                )
            )
            .limit(20)
        )
        pages = result.scalars().all()

        if not pages:
            print("âœ… ĞĞµÑ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ´Ğ»Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸.")
            return

        for page in pages:
            print(f"ğŸ§¼ Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°: {page.url}")
            if page.raw_author and not page.clean_author and not page.author_needs_review:
                page.clean_author = await gpt_clean(page.raw_author, prompt, model)

            if page.raw_date and not page.clean_date and not page.date_needs_review:
                try:
                    parsed_date = datetime.fromisoformat(page.raw_date)
                    page.clean_date = parsed_date
                except Exception:
                    page.date_needs_review = True

            if page.raw_category and not page.clean_category and not page.category_needs_review:
                page.clean_category = await gpt_clean(page.raw_category, prompt, model)

            session.add(page)

        await session.commit()
        print("âœ… Ğ’ÑĞµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹.")

if __name__ == "__main__":
    dataset_id = int(sys.argv[1]) if len(sys.argv) > 1 else 1
    asyncio.run(process_pages(dataset_id))

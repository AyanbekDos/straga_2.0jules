import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import asyncio
from sqlalchemy.future import select
from bs4 import BeautifulSoup
import trafilatura

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ‘Ğ” Ğ¸Ğ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹
from models.models import Page
from core.db import async_session_maker

def extract_clean_text(html: str) -> str:
    # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· trafilatura (Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚)
    clean = trafilatura.extract(html)
    if clean and len(clean.strip()) > 50:
        return clean.strip()
    # Fallback: BeautifulSoup
    soup = BeautifulSoup(html, 'lxml')
    text = soup.get_text(separator=' ', strip=True)
    return text if len(text) > 50 else ""

async def clean_pages(dataset_id: int, batch_size: int = 20):
    async with async_session_maker() as session:
        # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ²ÑĞµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ Ğ¿ÑƒÑÑ‚Ñ‹Ğ¼ clean_text Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ
        result = await session.execute(
            select(Page)
            .where(Page.clean_text.is_(None))
            .where(Page.link.has(dataset_id=dataset_id))
            .limit(batch_size)
        )
        pages = result.scalars().all()
        if not pages:
            print("ğŸ” ĞĞµÑ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ´Ğ»Ñ Ñ‡Ğ¸ÑÑ‚ĞºĞ¸.")
            return

        for page in pages:
            clean = extract_clean_text(page.raw_html)
            if clean:
                page.clean_text = clean
                print(f"âœ… cleaned: {page.url[:60]}")
            else:
                print(f"âš ï¸ Ğ¿ÑƒÑÑ‚Ğ¾: {page.url[:60]}")

        await session.commit()

if __name__ == "__main__":
    import sys
    dataset_id = int(sys.argv[1]) if len(sys.argv) > 1 else 1
    asyncio.run(clean_pages(dataset_id))

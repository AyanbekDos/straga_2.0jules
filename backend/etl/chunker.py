import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import asyncio
from sqlalchemy.future import select
from semantic_text_splitter import TextSplitter
import re

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ‘Ğ” Ğ¸Ğ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹
from models.models import Page, Chunk, Link, DatasetSettings
from core.db import async_session_maker

def flatten(text):
    return re.sub(r'\s+', ' ', text.replace('\n', ' ')).strip()

async def get_chunk_settings(session, dataset_id):
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ‡Ğ°Ğ½ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· dataset_settings
    result = await session.execute(
        select(DatasetSettings).where(DatasetSettings.dataset_id == dataset_id)
    )
    settings = result.scalar_one_or_none()
    chunk_size = settings.chunk_size if settings and settings.chunk_size else 512
    chunk_overlap = settings.chunk_overlap if settings and settings.chunk_overlap else 50
    return chunk_size, chunk_overlap

async def chunk_texts(dataset_id: int):
    async with async_session_maker() as session:
        chunk_size, chunk_overlap = await get_chunk_settings(session, dataset_id)
        print(f"ğŸ§© chunk_size={chunk_size}, chunk_overlap={chunk_overlap}")

        # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ²ÑĞµ pages Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼ clean_text, Ğ³Ğ´Ğµ ĞµÑ‰Ğµ Ğ½ĞµÑ‚ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²
        result = await session.execute(
            select(Page)
            .join(Link)
            .where(Link.dataset_id == dataset_id)
            .where(Page.clean_text.isnot(None))
        )
        pages = result.scalars().all()
        if not pages:
            print("âš ï¸ ĞĞµÑ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ´Ğ»Ñ Ñ‡Ğ°Ğ½ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.")
            return

        splitter = TextSplitter.from_tiktoken_model("gpt-3.5-turbo", chunk_size)
        total_chunks = 0

        for page in pages:
            text = flatten(page.clean_text)
            chunks = splitter.chunks(text)
            for idx, chunk_text in enumerate(chunks):
                session.add(Chunk(
                    page_id=page.id,
                    chunk_index=idx,
                    chunk_text=chunk_text
                ))
            total_chunks += len(chunks)
            print(f"âœ… {page.url[:50]} â€” Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²: {len(chunks)}")
            await session.commit()  # ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ½ĞµÑÑ‚Ğ¸ Ğ·Ğ° Ñ†Ğ¸ĞºĞ» Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸

        print(f"\nâœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²: {total_chunks}")

if __name__ == "__main__":
    import sys
    dataset_id = int(sys.argv[1]) if len(sys.argv) > 1 else 1
    asyncio.run(chunk_texts(dataset_id))
